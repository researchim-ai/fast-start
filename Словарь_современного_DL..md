**Deep Learning – Глубокое обучение**  
Область машинного обучения, использующая нейронные сети с множеством слоёв для решения сложных задач.

**Neural Network – Нейронная сеть**  
Модель, состоящая из взаимосвязанных "нейронов" (узлов), имитирующих работу человеческого мозга для обработки информации.

**Perceptron – Перцептрон**  
Базовый элемент нейронной сети, представляющий собой простейшую модель нейрона, который принимает входные данные, обрабатывает их и выдает результат.

**Activation Function – Функция активации**  
Функция, применяемая к сумме взвешенных входов нейрона, позволяющая ввести нелинейность в модель (например, ReLU, sigmoid, tanh).

**Loss Function – Функция потерь**  
Мера ошибки, показывающая разницу между предсказанными и истинными значениями, которую модель стремится минимизировать.

**Gradient Descent – Метод градиентного спуска**  
Алгоритм оптимизации, позволяющий корректировать веса модели в направлении наискорейшего уменьшения функции потерь.

**Backpropagation – Обратное распространение ошибки**  
Метод расчёта градиентов, используемый для обновления весов нейронной сети после каждого прохода обучения.

**Convolutional Neural Network (CNN) – Сверточная нейронная сеть**  
Архитектура нейронных сетей, оптимизированная для обработки изображений и обнаружения локальных признаков посредством свёрток.

**Recurrent Neural Network (RNN) – Рекуррентная нейронная сеть**  
Модель, способная учитывать последовательность данных, что делает её полезной для работы с временными рядами и текстами.

**Overfitting – Переобучение**  
Ситуация, когда модель слишком хорошо подстраивается под тренировочные данные, теряя способность обобщать на новые примеры.

**Underfitting – Недообучение**  
Состояние, при котором модель недостаточно сложна для захвата закономерностей в данных, что приводит к плохой производительности как на тренировочных, так и на тестовых данных.

**Regularization – Регуляризация**  
Набор техник (например, L1, L2, Dropout), направленных на уменьшение переобучения модели.

**Dropout – Отбрасывание**  
Метод регуляризации, при котором случайно исключаются некоторые нейроны в процессе обучения, чтобы модель не стала чрезмерно зависеть от отдельных путей.

**Batch Normalization – Нормализация пакета**  
Техника, позволяющая стабилизировать и ускорять обучение за счет нормализации входов каждого слоя в мини-пакетах данных.

**Learning Rate – Скорость обучения**  
Гиперпараметр, определяющий величину шага при обновлении весов модели в процессе оптимизации.

**Epoch – Эпоха**  
Один полный проход по всему набору тренировочных данных.

**Mini-Batch – Мини-пакет**  
Небольшая часть тренировочного набора данных, используемая для одного шага обновления весов, что помогает ускорить обучение и сделать его более стабильным.

**Weight Initialization – Инициализация весов**  
Процесс задания начальных значений весов нейронной сети, что существенно влияет на скорость и качество обучения.

**Transfer Learning – Трансферное обучение**  
Метод, при котором модель, обученная на одном наборе данных, адаптируется для решения другой, но смежной задачи.

**Autoencoder – Автоэнкодер**  
Нейронная сеть, используемая для обучения сжатого представления данных (например, для снижения размерности или обнаружения аномалий).

**Generative Adversarial Network (GAN) – Генеративно-состязательная сеть**  
Архитектура, состоящая из двух конкурирующих нейронных сетей (генератора и дискриминатора), что позволяет создавать новые, реалистичные данные.

**Reinforcement Learning – Обучение с подкреплением**  
Область машинного обучения, где агент обучается на основе обратной связи (подкреплений) от окружающей среды.

**Hyperparameters – Гиперпараметры**  
Параметры, задаваемые перед началом обучения модели (например, скорость обучения, размер мини-пакета, число слоев), которые определяют её архитектуру и процесс обучения.

**Feature Extraction – Выделение признаков**  
Процесс преобразования исходных данных в информативное представление, которое упрощает задачу обучения модели.

**Embedding – Встраивание**  
Метод представления дискретных данных (например, слов) в виде непрерывных векторов, что позволяет моделям работать с ними эффективнее.
