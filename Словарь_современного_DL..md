## Общий DL

**Deep Learning – Глубокое обучение**  
Область машинного обучения, использующая нейронные сети с множеством слоёв для решения сложных задач.

**Neural Network – Нейронная сеть**  
Модель, состоящая из взаимосвязанных "нейронов" (узлов), имитирующих работу человеческого мозга для обработки информации.

**Perceptron – Перцептрон**  
Базовый элемент нейронной сети, представляющий собой простейшую модель нейрона, который принимает входные данные, обрабатывает их и выдает результат.

**Activation Function – Функция активации**  
Функция, применяемая к сумме взвешенных входов нейрона, позволяющая ввести нелинейность в модель (например, ReLU, sigmoid, tanh).

**Loss Function – Функция потерь**  
Мера ошибки, показывающая разницу между предсказанными и истинными значениями, которую модель стремится минимизировать.

**Gradient Descent – Метод градиентного спуска**  
Алгоритм оптимизации, позволяющий корректировать веса модели в направлении наискорейшего уменьшения функции потерь.

**Backpropagation – Обратное распространение ошибки**  
Метод расчёта градиентов, используемый для обновления весов нейронной сети после каждого прохода обучения.

**Convolutional Neural Network (CNN) – Сверточная нейронная сеть**  
Архитектура нейронных сетей, оптимизированная для обработки изображений и обнаружения локальных признаков посредством свёрток.

**Recurrent Neural Network (RNN) – Рекуррентная нейронная сеть**  
Модель, способная учитывать последовательность данных, что делает её полезной для работы с временными рядами и текстами.

**Overfitting – Переобучение**  
Ситуация, когда модель слишком хорошо подстраивается под тренировочные данные, теряя способность обобщать на новые примеры.

**Underfitting – Недообучение**  
Состояние, при котором модель недостаточно сложна для захвата закономерностей в данных, что приводит к плохой производительности как на тренировочных, так и на тестовых данных.

**Regularization – Регуляризация**  
Набор техник (например, L1, L2, Dropout), направленных на уменьшение переобучения модели.

**Dropout – Отбрасывание**  
Метод регуляризации, при котором случайно исключаются некоторые нейроны в процессе обучения, чтобы модель не стала чрезмерно зависеть от отдельных путей.

**Batch Normalization – Нормализация пакета**  
Техника, позволяющая стабилизировать и ускорять обучение за счет нормализации входов каждого слоя в мини-пакетах данных.

**Learning Rate – Скорость обучения**  
Гиперпараметр, определяющий величину шага при обновлении весов модели в процессе оптимизации.

**Epoch – Эпоха**  
Один полный проход по всему набору тренировочных данных.

**Mini-Batch – Мини-пакет**  
Небольшая часть тренировочного набора данных, используемая для одного шага обновления весов, что помогает ускорить обучение и сделать его более стабильным.

**Weight Initialization – Инициализация весов**  
Процесс задания начальных значений весов нейронной сети, что существенно влияет на скорость и качество обучения.

**Transfer Learning – Трансферное обучение**  
Метод, при котором модель, обученная на одном наборе данных, адаптируется для решения другой, но смежной задачи.

**Autoencoder – Автоэнкодер**  
Нейронная сеть, используемая для обучения сжатого представления данных (например, для снижения размерности или обнаружения аномалий).

**Generative Adversarial Network (GAN) – Генеративно-состязательная сеть**  
Архитектура, состоящая из двух конкурирующих нейронных сетей (генератора и дискриминатора), что позволяет создавать новые, реалистичные данные.

**Reinforcement Learning – Обучение с подкреплением**  
Область машинного обучения, где агент обучается на основе обратной связи (подкреплений) от окружающей среды.

**Hyperparameters – Гиперпараметры**  
Параметры, задаваемые перед началом обучения модели (например, скорость обучения, размер мини-пакета, число слоев), которые определяют её архитектуру и процесс обучения.

**Feature Extraction – Выделение признаков**  
Процесс преобразования исходных данных в информативное представление, которое упрощает задачу обучения модели.

**Embedding – Встраивание**  
Метод представления дискретных данных (например, слов) в виде непрерывных векторов, что позволяет моделям работать с ними эффективнее.


## LLM - большие языковые модели

**LLM (Large Language Model) – Большая языковая модель**  
Комплексная нейросеть, обученная на огромном количестве текстовых данных для генерации, анализа и понимания естественного языка.

**Pretraining – Предобучение**  
Этап обучения модели на большом корпусе текстов без специализированной разметки. Здесь модель учится основным языковым закономерностям и структурам.

**Fine-tuning – Тонкая настройка**  
Доработка предобученной модели на более специализированном наборе данных с целью улучшения производительности в конкретных задачах.

**Token – Токен**  
Минимальная единица текста (слово, часть слова или символ), с которой работает модель. Токены используются для разбивки и обработки текстовой информации.

**Prompt – Промт/Запрос/Подсказка**  
Текст, который подается на вход модели для генерации ответа. Правильно сформулированный запрос может значительно улучшить качество ответа.

**Context Window – Контекстное окно**  
Ограничение на количество токенов, которое модель может одновременно учитывать при генерации ответа. Размер контекстного окна влияет на способность модели "помнить" ранее введённую информацию.

**In-context Learning – Обучение в контексте**  
Способность модели адаптироваться к задаче, используя примеры, предоставленные непосредственно в запросе, без необходимости дополнительного обучения.

**Zero-shot Learning – Обучение без примеров**  
Способность модели выполнять новую задачу, опираясь исключительно на полученное описание задачи, без предоставления примеров в запросе.

**Few-shot Learning – Обучение с несколькими примерами**  
Метод, при котором для демонстрации задачи в запросе предоставляется несколько примеров, что помогает модели лучше понять требуемый формат и контекст.

**Chain-of-thought Reasoning – Цепочка рассуждений**  
Техника, позволяющая модели пошагово описывать процесс рассуждения при решении задачи, что улучшает прозрачность и может привести к более точным выводам.
Пример: при решении сложной математической задачи модель может последовательно описывать, как она пришла к финальному ответу.

**Decoding – Декодирование**  
Процесс генерации текста из скрытых представлений модели. Различные алгоритмы декодирования (например, жадный поиск, beam search) влияют на качество и разнообразие ответов.

**Temperature – Температура**  
Гиперпараметр, управляющий случайностью в процессе генерации текста. Низкая температура делает вывод более детерминированным, высокая – разнообразнее, но менее предсказуемым.

**Beam Search – Поиск по лучу**  
Алгоритм, который рассматривает несколько вариантов продолжения текста одновременно, выбирая наиболее вероятные. Это помогает найти более оптимальный ответ по сравнению с жадным поиском.

**Self-supervised Learning – Самостоятельное обучение**  
Метод, при котором модель обучается на основе информации, содержащейся в данных, без явной разметки, извлекая закономерности и структуры из сырого текста.

**Prompt Engineering – Проектирование запросов**  
Искусство составления оптимальных запросов для получения от модели наиболее точного и полезного ответа. Это включает выбор слов, форматирование и структуру запроса.

**Alignment – Выравнивание (согласование)**  
Процесс адаптации модели так, чтобы её выводы соответствовали человеческим ценностям, этическим нормам и ожиданиям. Это может включать как тонкую настройку, так и дополнительные этапы обучения.

**Hallucination – Галлюцинация**  
Ситуация, когда модель генерирует фактическую информацию, не основанную на обучающих данных или реальности. Такие ошибки могут возникать при недостатке контекста или при неверной интерпретации запроса.

**Reinforcement Learning from Human Feedback (RLHF) – Обучение с подкреплением на основе обратной связи от человека**  
Метод дообучения, при котором модель корректируется на основе оценок и замечаний реальных пользователей или экспертов, что позволяет улучшить её поведение и снизить количество ошибок.
