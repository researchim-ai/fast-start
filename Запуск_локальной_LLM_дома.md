Если вам просто поднять LLM дома в чатовом приложении, то есть следующие чат-приложения для разворачивания LLM:

## LMStudio  
https://lmstudio.ai/  
документация - https://lmstudio.ai/docs/basics

### Инструкция по установке (актуально для LMStudio версии 0.3.8)
Выбираем для своей платформы - Windows/Linux/MacOS  
После установки нажимаем - Get Your first LLM.  
Вначале предложат установить Llama 3.2 1B. Это маленькая и относительно бесполезная моделька, но можно установить в тестовых целях (особенно если у Вас нет видеокарты или видеокарта с < 4GB видеопамяти).  

Нажимаем **Download 1.32 GB** - будет скачана Llama 3.2 1B (конкретно https://huggingface.co/hugging-quants/Llama-3.2-1B-Instruct-Q8_0-GGUF)  

После загрузки нажимаем Start New Chat, появится окно чата, но модель пока не развернута на компьютере. Запускать модельку можно как на видеокарте (GPU) так и на процессоре (CPU - будет работать медленнее).  Если модель сможет быть загруженана GPU (полностью или частично), то загрузится, если нет - то на CPU.  
Нажимаем сверху **Select a model to load** (в отдельной специальной строчке написано) и выбираем **Llama 3.2 1B Instruct** которая ранее была загружена.  
После того как модель будет развернута можно писать в чат - **Привет**, модель Вам ответит. Можно общаться. (но моделька довольно глупая, нужно бы найти что-то получше)

### Выбираем модельку для русского языка

![image](https://github.com/user-attachments/assets/57f6227d-8a3f-4a43-817e-679d0ee04b40)
 
Предлагаю запустить модельку на 2 миллиарда параметров **Vikhr Gemma 2 2B**: будет загружена отсюда https://huggingface.co/Vikhrmodels/Vikhr-Gemma-2B-instruct-GGUF  
Это специально дотренированная на русский язык модель.  
Для этого в строке поиска моделей сверху напишите **vikhr gemma 2B** после чего она появится в меню. Выберете ее и нажмите **Download**. Когда загрузится нажимаем **Use in New Chat** и общаемся.

![image](https://github.com/user-attachments/assets/11c20096-99b2-41dc-98e7-0aa0cda47fae)

Можно выбрать разные квантированные версии - по-умолчанию предлагается Q4_0, но я бы рекомендовал версию **Q6_K**.

### Ставим модельку способную к размышлениям

вначале убедитесь что у вас обновлен **Runtime**, для этого откройте настройки (справа в самом низу нажать на шестеренку)

![image](https://github.com/user-attachments/assets/a063b876-af39-43d0-9c86-0b5432e24842)

затем откройте вкладку меню **Runtimes** и проследите что установлен минимум **v1.9.2** для всех версий (с более ранними версиями модельки DeepSeek R1 Distill у меня отказались запускаться), если версия ниже, то нажмите **Update** на кнопочках.

<img width="1275" alt="image" src="https://github.com/user-attachments/assets/20ba6cb5-175d-4654-b625-48745192124a" />

Пишем в строке поиска **deepseek r1** и получаем список моделек которые умеют рассуждать.

![image](https://github.com/user-attachments/assets/47806e97-0ebd-423f-97f8-dd1c5ac647be)

Я взял первую же **DeepSeek R1 Distill (Qwen 7B)** официально выпущенную командой DeepSeek.

Далее пишем ему например "Порассуждай" и увидем что в окошечке **Thoughts** он немножко поговорит сам с собой

![image](https://github.com/user-attachments/assets/610b2edb-f72e-437e-a919-b99b83e78932)

Общаемся :)
